# üè† House Price Prediction Project

A clean, educational machine learning project for predicting house prices using the Ames Housing dataset. This project focuses on simplicity, clear explanations, and best practices for beginners and intermediate data scientists.

## üìã Table of Contents
- [Overview](#overview)
- [Dataset](#dataset)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Usage](#usage)
- [Methodology](#methodology)
- [Results](#results)
- [Key Learnings](#key-learnings)
- [Contributing](#contributing)

## üéØ Overview

This project demonstrates a complete machine learning workflow for regression tasks, specifically predicting house prices. The approach emphasizes:

- **Simplicity**: Clean, understandable code with extensive explanations
- **Best Practices**: Proper train-test splits, avoiding data leakage
- **Educational Value**: Step-by-step explanations of each decision
- **Practical Results**: Achieved 91.2% R¬≤ with Random Forest model

### üéì Learning Objectives
- Understand the complete ML pipeline from data cleaning to model evaluation
- Learn when to use linear vs non-linear models
- Master feature engineering and encoding techniques
- Implement proper model validation strategies

## üìä Dataset

**Source**: Ames Housing Dataset  
**Size**: 2,930 houses with 82 features  
**Target**: SalePrice (house prices in dollars)  
**Features**: Mix of numerical (lot size, year built) and categorical (neighborhood, house style)

### Key Statistics
- **Price Range**: $12,789 - $755,000
- **Average Price**: $180,796
- **Features After Cleaning**: 76 features
- **Missing Values**: Handled through simple imputation strategies

## üìÅ Project Structure

```
HousePrice Prediction/
‚îú‚îÄ‚îÄ README.md                          # This file
‚îú‚îÄ‚îÄ requirements.txt                   # Python dependencies
‚îú‚îÄ‚îÄ learning_guide.md                  # Additional learning resources
‚îú‚îÄ‚îÄ Project_Implementing_Steps         # Project planning notes
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ raw/
‚îÇ       ‚îî‚îÄ‚îÄ AmesHousing.csv           # Original dataset
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ clean_house_price_model.ipynb # Main analysis notebook
‚îî‚îÄ‚îÄ hpenv/                            # Virtual environment
```

## üöÄ Installation

### Prerequisites
- Python 3.8 or higher
- pip package manager

### Setup Instructions

1. **Clone or download the project**
   ```bash
   cd "HousePrice Prediction"
   ```

2. **Create virtual environment** (recommended)
   ```bash
   python -m venv hpenv
   source hpenv/bin/activate  # On Windows: hpenv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Launch Jupyter Notebook**
   ```bash
   jupyter notebook
   ```

5. **Open the main notebook**
   Navigate to `notebooks/clean_house_price_model.ipynb`

## üíª Usage

### Running the Analysis

1. **Open the notebook**: `notebooks/clean_house_price_model.ipynb`
2. **Run all cells** to reproduce the complete analysis
3. **Follow along** with the detailed explanations in each section

### Key Notebook Sections

1. **Data Loading & Exploration** - Understanding the dataset
2. **Data Cleaning** - Handling missing values and outliers
3. **Feature Engineering** - Converting categorical data to numerical
4. **Train-Test Split** - Proper data splitting strategies
5. **Model Building** - Linear Regression vs Random Forest
6. **Model Evaluation** - Performance metrics and interpretation

## üî¨ Methodology

### Data Preprocessing
- **Missing Value Treatment**: Drop features >50% missing, impute others
- **Feature Encoding**: Label encoding for categorical variables
- **No Scaling Applied**: Tree-based models don't require feature scaling

### Model Selection
- **Linear Regression**: Baseline model for interpretability
- **Random Forest**: Non-linear model for complex relationships
- **Evaluation Metrics**: R¬≤ Score and RMSE for performance assessment

### Validation Strategy
- **80/20 Train-Test Split**: Standard split with random state for reproducibility
- **No Data Leakage**: Encoding performed before splitting (safe with label encoding)
- **Overfitting Check**: Comparing training vs test performance

## üìà Results

### Model Performance Comparison

| Model | Test R¬≤ | Test RMSE | Interpretation |
|-------|---------|-----------|----------------|
| **Linear Regression** | 0.862 | $33,258 | Explains 86.2% of price variation |
| **Random Forest** | 0.912 | $26,606 | Explains 91.2% of price variation |

### Key Insights

üèÜ **Winner**: Random Forest outperformed Linear Regression
- **5.8% better R¬≤** score
- **$6,652 lower RMSE** (prediction error)
- Better at capturing **non-linear relationships** and **feature interactions**

‚ö†Ô∏è **Overfitting Note**: Random Forest showed some overfitting (98.3% training vs 91.2% test R¬≤)

### Practical Impact
For a $200,000 house:
- **Linear Regression**: Prediction range $166,742 - $233,258
- **Random Forest**: Prediction range $173,394 - $226,606

## üß† Key Learnings

### When to Use Non-Linear Models
‚úÖ **Use Random Forest/Non-Linear when**:
- Complex relationships between features
- Feature interactions matter (location + size + age)
- Accuracy is more important than interpretability
- Sufficient training data available

‚úÖ **Use Linear Regression when**:
- Need interpretable results
- Simple, proportional relationships
- Limited training data
- Fast deployment required

### Feature Scaling Insights
- **Not needed** for tree-based models (Random Forest)
- **Optional** for Linear Regression without regularization
- **Critical** for distance-based models (KNN, SVM, Neural Networks)

### Data Science Best Practices Demonstrated
1. **Always split data** before any preprocessing that could cause leakage
2. **Label encoding is safe** before splitting (doesn't use target variable)
3. **Compare multiple models** to find the best approach
4. **Check for overfitting** by comparing training vs test performance
5. **Use practical metrics** (RMSE in dollars, not just R¬≤)

## üîß Technical Requirements

```python
pandas==2.0.3
numpy==1.24.3
matplotlib==3.7.2
seaborn==0.12.2
scikit-learn==1.3.0
jupyter==1.0.0
```

## ü§ù Contributing

This project is designed for educational purposes. Feel free to:

1. **Fork the repository**
2. **Try different models** (XGBoost, Neural Networks)
3. **Experiment with feature engineering**
4. **Add visualizations**
5. **Improve documentation**

### Suggestions for Extensions
- Add feature importance analysis
- Implement cross-validation
- Try ensemble methods
- Add model explainability (SHAP values)
- Create a simple web interface

